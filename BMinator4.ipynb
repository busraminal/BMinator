{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6flB_41WgHbn"},"outputs":[],"source":[" import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","import re\n"]},{"cell_type":"code","source":["import os\n","\n","for fname in os.listdir(\"/content\"):\n","    print(fname)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHQo3A62iHfS","executionInfo":{"status":"ok","timestamp":1747607650825,"user_tz":-180,"elapsed":49,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"ce2c202c-8631-4dd7-82e6-eb74dce34b01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".config\n","sample_data\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"EOE7l3zfiWU3","executionInfo":{"status":"ok","timestamp":1747607791058,"user_tz":-180,"elapsed":79440,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"bd6db240-11d2-40d5-81d2-c3136c0bf42c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-f8c4ce42-cb71-44f2-9101-45823fbac9c4\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f8c4ce42-cb71-44f2-9101-45823fbac9c4\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving training_data_20000_gpt_like.json to training_data_20000_gpt_like.json\n"]}]},{"cell_type":"code","source":["import os\n","print(os.listdir(\"/content\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWABDoN_iYdq","executionInfo":{"status":"ok","timestamp":1747607793846,"user_tz":-180,"elapsed":39,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"05ab3c57-3bd2-4606-fd40-355578b3afd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['.config', 'training_data_20000_gpt_like.json', 'sample_data']\n"]}]},{"cell_type":"code","source":["json_path = \"/content/training_data_20000_gpt_like.json\"\n"],"metadata":{"id":"osMb8B3riblb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import torch\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","\n","\n","json_path = \"/content/training_data_20000_gpt_like.json\"\n","\n","all_words = []\n","with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        obj = json.loads(line)\n","        words = obj[\"text\"].strip().split()\n","        all_words.extend(words)\n","\n","\n","vocab_size = 5000\n","word_counts = Counter(all_words)\n","most_common = word_counts.most_common(vocab_size - 2)  # 0: PAD, 1: UNK\n","word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n","for i, (word, _) in enumerate(most_common, start=2):\n","    word2idx[word] = i\n","idx2word = {i: w for w, i in word2idx.items()}\n","\n","\n","with open(\"word_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(word2idx, f, ensure_ascii=False, indent=2)\n","\n","\n","def words_to_tensor(text, max_len=60):\n","    tokens = text.strip().split()\n","    ids = [word2idx.get(w, 1) for w in tokens]  # 1: UNK\n","    ids += [0] * (max_len - len(ids))\n","    return ids[:max_len]\n","\n","MAX_LEN = 60\n","all_data = []\n","\n","with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        obj = json.loads(line)\n","        vec = obj[\"vector\"]\n","        ids = words_to_tensor(obj[\"text\"], MAX_LEN)\n","        all_data.append(json.dumps({\"vector\": vec, \"text_ids\": ids}))\n","\n","\n","part_size = len(all_data) // 4\n","for i in range(4):\n","    part = all_data[i*part_size:(i+1)*part_size]\n","    with open(f\"worddata_part{i+1}.json\", \"w\", encoding=\"utf-8\") as f:\n","        for line in part:\n","            f.write(line + \"\\n\")\n","\n","print(\"‚úÖ Word-level veri hazƒ±r ve 4 par√ßaya b√∂l√ºnd√º!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-LB0YMDisrf","executionInfo":{"status":"ok","timestamp":1747607845513,"user_tz":-180,"elapsed":1492,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"db912f40-4ef4-4554-fd31-7224d5fbf2f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Word-level veri hazƒ±r ve 4 par√ßaya b√∂l√ºnd√º!\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","\n","\n","with open(\"word_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n","    word2idx = json.load(f)\n","vocab_size = len(word2idx)\n","\n","\n","class WordPersonaDataset(Dataset):\n","    def __init__(self, path):\n","        self.data = []\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                obj = json.loads(line)\n","                vec = torch.tensor(obj[\"vector\"]).float()\n","                ids = torch.tensor(obj[\"text_ids\"]).long()\n","                self.data.append((vec, ids))\n","    def __len__(self): return len(self.data)\n","    def __getitem__(self, idx): return self.data[idx]\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def forward(self, Q, K, V):\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / Q.size(-1) ** 0.5\n","        return torch.matmul(torch.softmax(scores, dim=-1), V)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, dim, heads):\n","        super().__init__()\n","        self.q = nn.Linear(dim, dim)\n","        self.k = nn.Linear(dim, dim)\n","        self.v = nn.Linear(dim, dim)\n","        self.out = nn.Linear(dim, dim)\n","        self.heads = heads\n","        self.dim = dim // heads\n","        self.attn = ScaledDotProductAttention()\n","\n","    def forward(self, x):\n","        B, T, D = x.size()\n","        Q = self.q(x).view(B, T, self.heads, self.dim).transpose(1, 2)\n","        K = self.k(x).view(B, T, self.heads, self.dim).transpose(1, 2)\n","        V = self.v(x).view(B, T, self.heads, self.dim).transpose(1, 2)\n","        out = self.attn(Q, K, V).transpose(1, 2).contiguous().view(B, T, D)\n","        return self.out(out)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, dim, heads, ff_dim):\n","        super().__init__()\n","        self.attn = MultiHeadAttention(dim, heads)\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.ff = nn.Sequential(nn.Linear(dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, dim))\n","        self.norm2 = nn.LayerNorm(dim)\n","\n","    def forward(self, x):\n","        x = self.norm1(x + self.attn(x))\n","        return self.norm2(x + self.ff(x))\n","\n","\n","class WordLevelModel(nn.Module):\n","    def __init__(self, persona_dim, vocab_size, emb_dim, heads, ff_dim, layers):\n","        super().__init__()\n","        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n","        self.persona_proj = nn.Sequential(nn.Linear(persona_dim, emb_dim), nn.ReLU())\n","        self.blocks = nn.ModuleList([TransformerBlock(emb_dim, heads, ff_dim) for _ in range(layers)])\n","        self.out = nn.Linear(emb_dim, vocab_size)\n","\n","    def forward(self, persona, seq):\n","        x = self.word_emb(seq)\n","        p = self.persona_proj(persona).unsqueeze(1)\n","        x = x + p\n","        for block in self.blocks:\n","            x = block(x)\n","        return self.out(x)\n","\n","\n","EMBED_DIM = 256\n","FF_DIM = 512\n","HEADS = 8\n","LAYERS = 4\n","MAX_LEN = 60\n","EPOCHS = 30\n","BATCH_SIZE = 8\n","LR = 5e-4\n","PATIENCE = 5\n","\n","model = WordLevelModel(25, vocab_size, EMBED_DIM, HEADS, FF_DIM, LAYERS)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.CrossEntropyLoss()\n","\n","best_loss = float(\"inf\")\n","patience_count = 0\n","\n","\n","for part in range(1, 5):\n","    print(f\"\\nüß© Part {part}/4 ba≈ülƒ±yor...\")\n","    dataset = WordPersonaDataset(f\"worddata_part{part}.json\")\n","    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    for epoch in range(EPOCHS):\n","        total_loss = 0\n","        model.train()\n","        for persona_vec, ids in loader:\n","            optimizer.zero_grad()\n","            output = model(persona_vec, ids[:, :-1])\n","            loss = criterion(output.reshape(-1, vocab_size), ids[:, 1:].reshape(-1))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        print(f\"üìò Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f}\")\n","        if (epoch + 1) % 10 == 0:\n","            torch.save(model.state_dict(), f\"wordlevel_part{part}_epoch{epoch+1}.pth\")\n","            print(\"üíæ Kaydedildi.\")\n","\n","        if best_loss - total_loss > 0.1:\n","            best_loss = total_loss\n","            patience_count = 0\n","            torch.save(model.state_dict(), f\"wordlevel_best_part{part}.pth\")\n","        else:\n","            patience_count += 1\n","            if patience_count >= PATIENCE:\n","                print(\"üõë Early Stopping\")\n","                break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QZ4HJnYjXMt","executionInfo":{"status":"ok","timestamp":1747608544782,"user_tz":-180,"elapsed":566890,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"b8731d0e-4642-4552-ffe9-836dd2718a75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üß© Part 1/4 ba≈ülƒ±yor...\n","üìò Epoch 1/30 | Loss: 148.2188\n","üìò Epoch 2/30 | Loss: 37.6365\n","üìò Epoch 3/30 | Loss: 31.6012\n","üìò Epoch 4/30 | Loss: 23.3262\n","üìò Epoch 5/30 | Loss: 12.2155\n","üìò Epoch 6/30 | Loss: 3.3135\n","üìò Epoch 7/30 | Loss: 2.1541\n","üìò Epoch 8/30 | Loss: 2.4175\n","üìò Epoch 9/30 | Loss: 1.7403\n","üìò Epoch 10/30 | Loss: 1.8015\n","üíæ Kaydedildi.\n","üìò Epoch 11/30 | Loss: 1.6903\n","üìò Epoch 12/30 | Loss: 1.7021\n","üìò Epoch 13/30 | Loss: 1.6711\n","üìò Epoch 14/30 | Loss: 1.6513\n","üõë Early Stopping\n","\n","üß© Part 2/4 ba≈ülƒ±yor...\n","üìò Epoch 1/30 | Loss: 33.2472\n","üõë Early Stopping\n","\n","üß© Part 3/4 ba≈ülƒ±yor...\n","üìò Epoch 1/30 | Loss: 20.0836\n","üõë Early Stopping\n","\n","üß© Part 4/4 ba≈ülƒ±yor...\n","üìò Epoch 1/30 | Loss: 14.2070\n","üõë Early Stopping\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_dim, num_heads, ff_dim):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.ff = nn.Sequential(\n","            nn.Linear(embed_dim, ff_dim),\n","            nn.ReLU(),\n","            nn.Linear(ff_dim, embed_dim)\n","        )\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        attn_output, _ = self.attn(x, x, x)\n","        x = self.norm1(x + attn_output)\n","        x = self.norm2(x + self.ff(x))\n","        return x\n","\n","class LifeStoryModel(nn.Module):\n","    def __init__(self, persona_dim, vocab_size, embed_dim, num_heads, ff_dim, num_layers):\n","        super().__init__()\n","        self.persona_proj = nn.Linear(persona_dim, embed_dim)\n","        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n","        self.transformer_blocks = nn.ModuleList([\n","            TransformerBlock(embed_dim, num_heads, ff_dim)\n","            for _ in range(num_layers)\n","        ])\n","        self.out = nn.Linear(embed_dim, vocab_size)\n","\n","    def forward(self, persona_vec, seq):\n","        seq_embed = self.word_embed(seq)\n","        persona_embed = self.persona_proj(persona_vec).unsqueeze(1)\n","        x = seq_embed + persona_embed\n","        for block in self.transformer_blocks:\n","            x = block(x)\n","        return self.out(x)\n","\n"],"metadata":{"id":"1zGs8NpGmC80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/deeppersona_weights/deeppersona_word_final.pth\")\n","print(\"‚úÖ Model ba≈üarƒ±yla kaydedildi.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRrpXiD8qDGm","executionInfo":{"status":"ok","timestamp":1747609728883,"user_tz":-180,"elapsed":50,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"adabe129-8a18-440c-e8c8-a5b676a00f4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Model ba≈üarƒ±yla kaydedildi.\n"]}]},{"cell_type":"code","source":["\n","model = LifeStoryModel(\n","    persona_dim=25,\n","    vocab_size=len(word2idx),\n","    embed_dim=192,\n","    num_heads=6,\n","    ff_dim=384,\n","    num_layers=3\n",")\n","\n","model.load_state_dict(torch.load(\n","    \"/content/drive/MyDrive/deeppersona_weights/deeppersona_word_final.pth\",\n","    map_location=\"cpu\"\n","))\n","model.eval()\n","print(\"‚úÖ Word-level model ba≈üarƒ±yla y√ºklendi.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2qnyQTgqFa2","executionInfo":{"status":"ok","timestamp":1747609738444,"user_tz":-180,"elapsed":53,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"8cb7c41b-4fbd-4896-a8d4-b395fb9030b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Word-level model ba≈üarƒ±yla y√ºklendi.\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/deeppersona_weights/deeppersona_word_final.pth\")\n","print(\"‚úÖ Word-level model ba≈üarƒ±yla kaydedildi.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxOzFUawqY8q","executionInfo":{"status":"ok","timestamp":1747609819618,"user_tz":-180,"elapsed":53,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"33eb7a0c-d3dc-4128-e96b-c54d5ac78df9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Word-level model ba≈üarƒ±yla kaydedildi.\n"]}]},{"cell_type":"code","source":["\n","model = LifeStoryModel(\n","    persona_dim=25,\n","    vocab_size=len(word2idx),\n","    embed_dim=192,\n","    num_heads=6,\n","    ff_dim=384,\n","    num_layers=3\n",")\n","\n","\n","model.load_state_dict(torch.load(\n","    \"/content/drive/MyDrive/deeppersona_weights/deeppersona_word_final.pth\",\n","    map_location=\"cpu\"\n","))\n","model.eval()\n","\n","print(\"‚úÖ Model ba≈üarƒ±yla y√ºklendi ve test moduna alƒ±ndƒ±.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SobZW0mjqcfS","executionInfo":{"status":"ok","timestamp":1747609832992,"user_tz":-180,"elapsed":51,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"1a0d6861-6371-42df-8180-24e21645845e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Model ba≈üarƒ±yla y√ºklendi ve test moduna alƒ±ndƒ±.\n"]}]},{"cell_type":"code","source":["import json\n","\n","with open(\"/content/worddata_part1.json\", encoding=\"utf-8\") as f:\n","    first_line = f.readline()\n","    data = json.loads(first_line)\n","    print(\"üì¶ JSON Anahtarlarƒ±:\", data.keys())\n","    print(\"üîé √ñrnek veri:\", data)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPzDeCfLscMH","executionInfo":{"status":"ok","timestamp":1747610355956,"user_tz":-180,"elapsed":19,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"dd85184d-e1af-46e4-d0e1-29cfa52dadb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì¶ JSON Anahtarlarƒ±: dict_keys(['vector', 'text_ids'])\n","üîé √ñrnek veri: {'vector': [0.483101, -0.510216, -0.720924, -0.79501, 0.481335, 0.090733, 0.180985, -0.936435, -0.81261, -0.534678, 0.204037, 0.12249, 0.432039, 0.40265, -0.16096, -0.101582, -0.443619, 0.738601, 0.517615, -0.680681, -0.154771, -0.444257, -0.569372, 0.526988, -0.795579], 'text_ids': [82, 3923, 3, 4, 88, 5, 2, 10, 6, 7, 11, 8, 34, 35, 36, 37, 38, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}]},{"cell_type":"code","source":["import torch\n","import json\n","\n","# S√∂zl√ºƒü√º y√ºkle\n","with open(\"/content/word_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n","    vocab = json.load(f)\n","    id2word = {int(v): k for k, v in vocab.items()}\n","\n","# Test √∂rneƒüini al\n","with open(\"/content/worddata_part1.json\", encoding=\"utf-8\") as f:\n","    example = json.loads(f.readline())\n","    test_vec = torch.tensor(example[\"vector\"]).unsqueeze(0).float()\n","    input_ids = example[\"text_ids\"]\n","\n","# Orijinal hikayeyi geri d√∂n√º≈üt√ºr (ID ‚Üí kelime)\n","words = [id2word[i] for i in input_ids if i in id2word]\n","original_text = \" \".join(words)\n","\n","print(\"‚úÖ Vekt√∂r ba≈üarƒ±yla y√ºklendi.\")\n","print(\"üìù Eƒüitimde kullanƒ±lan ger√ßek metin:\\n\", original_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"re_xorsgsvEV","executionInfo":{"status":"ok","timestamp":1747610433264,"user_tz":-180,"elapsed":60,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"57a22ae6-69f1-4e8e-a970-86a269b8a202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Vekt√∂r ba≈üarƒ±yla y√ºklendi.\n","üìù Eƒüitimde kullanƒ±lan ger√ßek metin:\n"," North Satrettinbury ≈üehrinde ya≈üayan 60 ya≈üƒ±nda bir kadƒ±n. Kendini genellikle mutlu hissediyor. K√º√ß√ºk ya≈ülardan beri yazƒ± yazmaya meraklƒ±ydƒ±. <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"]}]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","    input_ids = [word2idx[\"<BOS>\"]]\n","\n","    for _ in range(max_len):\n","        input_tensor = torch.tensor(input_ids).unsqueeze(0)  # (1, seq_len)\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]  # son token √ßƒ±ktƒ±sƒ±\n","            probs = torch.softmax(logits / temperature, dim=-1).squeeze()\n","            next_id = torch.multinomial(probs, num_samples=1).item()\n","\n","        if next_id == word2idx[\"<EOS>\"]:\n","            break\n","        input_ids.append(next_id)\n","\n","    return \" \".join([idx2word[i] for i in input_ids[1:]])  # <BOS> dahil deƒüil\n"],"metadata":{"id":"Ng0jI9WJtpIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word2idx[\"<BOS>\"] = len(word2idx)\n","word2idx[\"<EOS>\"] = len(word2idx)\n","word2idx[\"<PAD>\"] = len(word2idx)\n","\n","# Aynƒ± ≈üekilde idx2word de g√ºncellenmeli:\n","idx2word[word2idx[\"<BOS>\"]] = \"<BOS>\"\n","idx2word[word2idx[\"<EOS>\"]] = \"<EOS>\"\n","idx2word[word2idx[\"<PAD>\"]] = \"<PAD>\"\n"],"metadata":{"id":"lp-4IjkxuAUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","with open(\"word_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(word2idx, f, ensure_ascii=False, indent=2)\n"],"metadata":{"id":"ElWyEcdTuBHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# √ñzel tokenlar eksikse ekle\n","special_tokens = [\"<BOS>\", \"<EOS>\", \"<PAD>\"]\n","\n","for token in special_tokens:\n","    if token not in word2idx:\n","        word2idx[token] = len(word2idx)\n","        idx2word[word2idx[token]] = token\n"],"metadata":{"id":"hCExs7GTuQZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","    input_ids = [word2idx[\"<BOS>\"]]  # Ba≈ülangƒ±√ß tokenƒ±\n","\n","    for _ in range(max_len):\n","        input_tensor = torch.tensor(input_ids).unsqueeze(0)  # (1, seq_len)\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]  # son token √ßƒ±ktƒ±sƒ±\n","            probs = torch.softmax(logits / temperature, dim=-1).squeeze()\n","            next_id = torch.multinomial(probs, num_samples=1).item()\n","\n","        if next_id == word2idx[\"<EOS>\"]:\n","            break\n","        input_ids.append(next_id)\n","\n","    return \" \".join([idx2word[i] for i in input_ids[1:]])\n"],"metadata":{"id":"Ee1nIHHouSRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","\n","    vocab_size = len(word2idx)\n","    input_ids = [word2idx.get(\"<BOS>\", 0)]\n","\n","    for _ in range(max_len):\n","        input_tensor = torch.tensor(input_ids).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]\n","            probs = torch.softmax(logits / temperature, dim=-1).squeeze()\n","\n","        if torch.isnan(probs).any():\n","            probs = torch.ones_like(probs) / len(probs)\n","\n","        next_id = torch.multinomial(probs, num_samples=1).item()\n","\n","\n","        if next_id >= vocab_size or next_id < 0:\n","            next_id = word2idx.get(\"<EOS>\", 0)\n","\n","\n","        if idx2word.get(next_id, \"\") == \"<EOS>\":\n","            break\n","\n","        input_ids.append(next_id)\n","\n","\n","    return \" \".join([idx2word.get(i, \"\") for i in input_ids[1:] if i in idx2word])\n","\n","\n"],"metadata":{"id":"T6bb0UdIuy8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","\n","    vocab_size = len(word2idx)\n","    bos_id = word2idx.get(\"<BOS>\", 0)\n","    eos_id = word2idx.get(\"<EOS>\", vocab_size - 1)\n","\n","    input_ids = [bos_id]\n","\n","    for _ in range(max_len):\n","\n","        safe_input_ids = [i if i < vocab_size else word2idx.get(\"<UNK>\", 0) for i in input_ids]\n","        input_tensor = torch.tensor(safe_input_ids).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]  # son token √ßƒ±ktƒ±sƒ±\n","            probs = torch.softmax(logits / temperature, dim=-1).squeeze()\n","\n","\n","        if torch.isnan(probs).any():\n","            probs = torch.ones_like(probs) / len(probs)\n","\n","\n","        next_id = torch.multinomial(probs, num_samples=1).item()\n","\n","\n","        if next_id >= vocab_size or next_id < 0:\n","            next_id = eos_id\n","        if next_id == eos_id:\n","            break\n","\n","        input_ids.append(next_id)\n","\n","\n","    return \" \".join([idx2word.get(i, \"<UNK>\") for i in input_ids[1:] if i in idx2word])\n"],"metadata":{"id":"xoYufvw3yKUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Vocab size:\", len(word2idx))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nE7AA27Zy-wq","executionInfo":{"status":"ok","timestamp":1747612071099,"user_tz":-180,"elapsed":43,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"85fdd386-a1b7-4ba6-fcf6-94bb0968d560"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 5002\n"]}]},{"cell_type":"code","source":["# input_ids'in sƒ±nƒ±r dƒ±≈üƒ±na √ßƒ±kan deƒüerleri kontrol et\n","input_ids = [word2idx.get(\"<BOS>\", 0)]\n","for _ in range(50):\n","    if any(i >= len(word2idx) or i < 0 for i in input_ids):\n","        print(\" Hatalƒ± index bulundu:\", input_ids)\n","        break\n"],"metadata":{"id":"hH8lLTt4zBhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","\n","    vocab_size = len(word2idx)\n","    bos_id = word2idx.get(\"<BOS>\", 0)\n","    eos_id = word2idx.get(\"<EOS>\", vocab_size - 1)\n","    unk_id = word2idx.get(\"<UNK>\", 0)\n","\n","    input_ids = [bos_id]\n","\n","    for _ in range(max_len):\n","\n","        safe_input_ids = [i if i < vocab_size else unk_id for i in input_ids]\n","        input_tensor = torch.tensor(safe_input_ids).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]\n","            probs = torch.softmax(logits / temperature, dim=-1).squeeze()\n","\n","\n","        if torch.isnan(probs).any():\n","            probs = torch.ones_like(probs) / len(probs)\n","\n","\n","        next_id = torch.multinomial(probs, num_samples=1).item()\n","\n","\n","        if next_id >= vocab_size or next_id < 0:\n","            next_id = unk_id\n","\n","        if next_id == eos_id:\n","            break\n","\n","        input_ids.append(next_id)\n","\n","\n","    return \" \".join([idx2word.get(i, \"<UNK>\") for i in input_ids[1:]])\n"],"metadata":{"id":"BODlwXsr0Cw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unk_id = word2idx.get(\"<UNK>\", 0)\n","\n","safe_input_ids = []\n","for i in input_ids:\n","    if isinstance(i, int) and 0 <= i < vocab_size:\n","        safe_input_ids.append(i)\n","    else:\n","        safe_input_ids.append(unk_id)\n","\n"],"metadata":{"id":"pf8N-Yfv0dzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temperature = 0.8\n"],"metadata":{"id":"DZmI2M5N2j1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bos_id = word2idx.get(\"<BOS>\", 0)\n","eos_id = word2idx.get(\"<EOS>\", 1)\n","unk_id = word2idx.get(\"<UNK>\", 2)\n","vocab_size = len(word2idx)\n"],"metadata":{"id":"dfr9ZtSn4KbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"test_vec shape:\", test_vec.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JpvMBJcU4RDz","executionInfo":{"status":"ok","timestamp":1747613456715,"user_tz":-180,"elapsed":12,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"e2fcbe26-dd07-4f8c-9051-6b4212373d8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test_vec shape: torch.Size([1, 25])\n"]}]},{"cell_type":"code","source":["print(\"first few words in idx2word:\", list(idx2word.items())[:5])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fTxvucu4T1l","executionInfo":{"status":"ok","timestamp":1747613467659,"user_tz":-180,"elapsed":25,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"07b53629-1c13-45fb-8380-f003c78dc589"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["first few words in idx2word: [(0, '<PAD>'), (1, '<UNK>'), (2, 'bir'), (3, '≈üehrinde'), (4, 'ya≈üayan')]\n"]}]},{"cell_type":"code","source":["print(\"Embedding shape:\", model.word_embed.weight.shape)\n","print(\"Maximum used input id:\", max(input_ids))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlqBEmrb49aG","executionInfo":{"status":"ok","timestamp":1747613733608,"user_tz":-180,"elapsed":22,"user":{"displayName":"B√º≈üra Mina Al","userId":"09030843873886986105"}},"outputId":"d185007c-b336-4350-bc3a-eb2b09d6ee8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding shape: torch.Size([5000, 192])\n","Maximum used input id: 5000\n"]}]},{"cell_type":"code","source":["safe_input_ids = []\n","unk_id = word2idx.get(\"<UNK>\", 1)\n","\n","for i in input_ids:\n","    if isinstance(i, int) and 0 <= i < 5000:\n","        safe_input_ids.append(i)\n","    else:\n","        safe_input_ids.append(unk_id)\n"],"metadata":{"id":"4fPxqbFW5V1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_tensor = torch.tensor([safe_input_ids], dtype=torch.long)\n"],"metadata":{"id":"M36uj6wC5tzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","\n","    vocab_size = len(word2idx)\n","    bos_id = word2idx.get(\"<BOS>\", 0)\n","    eos_id = word2idx.get(\"<EOS>\", 1)\n","    unk_id = word2idx.get(\"<UNK>\", 2)\n","\n","    input_ids = [bos_id]\n","\n","    for _ in range(max_len):\n","\n","        input_tensor = torch.tensor([input_ids], dtype=torch.long)\n","\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]\n","            probs = torch.softmax(logits / temperature, dim=-1)\n","\n","\n","            if torch.isnan(probs).any() or probs.sum() == 0:\n","                probs = torch.ones_like(probs) / probs.shape[-1]\n","\n","\n","            if probs.dim() > 1:\n","                probs = probs.squeeze()\n","\n","\n","            next_id = torch.multinomial(probs, num_samples=1).item()\n","\n","\n","        if next_id >= vocab_size or next_id < 0:\n","            next_id = unk_id\n","\n","        if next_id == eos_id:\n","            break\n","\n","        input_ids.append(next_id)\n","\n","\n","    return \" \".join([idx2word.get(i, \"<UNK>\") for i in input_ids[1:]])\n","\n"],"metadata":{"id":"Ev_qoP-m5wk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_word_level(model, persona_vec, word2idx, idx2word, max_len=50, temperature=0.8):\n","    model.eval()\n","    vocab_size = len(word2idx)\n","    bos_id = word2idx.get(\"<BOS>\", 0)\n","    eos_id = word2idx.get(\"<EOS>\", 1)\n","    unk_id = word2idx.get(\"<UNK>\", 2)\n","\n","    input_ids = [bos_id]\n","    generated_words = []\n","\n","    for step in range(max_len):\n","\n","        input_ids = [i if 0 <= i < vocab_size else unk_id for i in input_ids]\n","        input_tensor = torch.tensor([input_ids], dtype=torch.long)\n","\n","        with torch.no_grad():\n","            logits = model(persona_vec, input_tensor)[:, -1, :]\n","            probs = torch.softmax(logits / temperature, dim=-1).squeeze()\n","\n","\n","        if torch.isnan(probs).any() or probs.sum() == 0:\n","            print(\"‚ö†Ô∏èprobs bozuldu, e≈üit daƒüƒ±lƒ±m atanƒ±yor.\")\n","            probs = torch.ones(vocab_size) / vocab_size\n","\n","        next_id = torch.multinomial(probs, 1).item()\n","\n","        if next_id >= vocab_size or next_id < 0:\n","            print(f\" next_id ge√ßersiz: {next_id}, <UNK> atanƒ±yor.\")\n","            next_id = unk_id\n","\n","        word = idx2word.get(next_id, \"<UNK>\")\n","        if next_id == eos_id:\n","            break\n","\n","        generated_words.append(word)\n","        input_ids.append(next_id)\n","\n","    return \" \".join(generated_words)\n","\n","\n"],"metadata":{"id":"9W0o4irB8V5O"},"execution_count":null,"outputs":[]}]}